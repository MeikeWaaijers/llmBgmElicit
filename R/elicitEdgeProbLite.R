#' Elicit Edge Inclusion Priors for Variable Pairs Using a LLM
#' that takes into account the remaining variables
#'
#'
#' This function uses a large language model (LLM) to evaluate whether conditional
#' associations (edges) exist between variable pairs in a Markov random Field graphical model. It optionally
#' includes study context and considers different orderings of remaining variables in the network
#' to estimate the prior probability of edge inclusion. Unlike the `elicitEdgeProb` function,
#' it does not include the decisions from the previous iterations for the remaining edges
#' in the prompt and is therefore cheaper to user. However it is also less accurate.
#'
#' @param context Optional character string. Describes the research context to inform the LLM's evaluation.
#' @param variable_list A character vector with at least three variable names.
#' @param LLM_model A character string specifying the model to use. One of: `"gpt-4o"`, `"gpt-4-turbo"`,
#'   `"gpt-3.5-turbo"`, `"mixtral"`, or `"llama-3"`.
#' @param update_key Logical. If `TRUE`, updates the API key used for the LLM call. Default is `FALSE`.
#' @param n_perm Optional integer. Number of sampled permutations of remaining variables. Ignored for small sets.
#' If `NULL`, two permutations are generated by default. The maximum is 50.
#' @param seed Integer. Random seed for reproducibility of the permutations. Default is `123`.
#' @param main_prompt Optional vector of character strings to be used as a system prompt for the LLM.
#' If `NULL`, a default system prompt is used. The prompt should be a single string.
#' @param display_progress Logical. If `TRUE`, displays progress messages during processing. Default is `TRUE`.
#' @param logprobs Logical. If `TRUE`, requests log probabilities from the LLM for the first token. Only available
#' for models that support logprobs (e.g., `gpt-4o`, `gpt-4-turbo`, `gpt-3.5-turbo`). Default is `TRUE`.
#' # if LLM_model does not support logprobs, this argument is ignored.
#' @return A list of class `"elicitEdgeProbLite"` containing:
#' \describe{
#'   \item{`relation_df`}{A data frame with columns `var1`, `var2`, and `prob`, containing the estimated
#'     prior probabilities of conditional associations between variable pairs.}
#'   \item{`raw_LLM`}{A data frame of all raw LLM prompt-response data and token probabilities.}
#'   \item{`arguments`}{A list of the function input arguments for reproducibility.}
#' }
#'
#' @details
#' The function examines all unique variable pairs and, for each, prompts the LLM
#' to assess the presence of a conditional association, factoring in remaining variables in different
#' permutations. If context is provided, it is included in the prompt to inform decisions.
#'
#' The returned edge inclusion probability reflects the average over all considered permutations
#' for each variable pair. The function handles both default and user-specified prompting structures.
#'
#' @examples
#' \dontrun{
#' result <- elicitEdgeProbLite(
#'   context = "This study examines the relationship between screen time,
#'   physical activity, and cardiovascular health.",
#'   variable_list = c("Screen Time", "Physical Activity", "Cardiovascular Health"),
#'   LLM_model = "gpt-4o"
#' )
#' print(result$relation_df)
#' }
#'
#' @export


elicitEdgeProbLite <- function(
    context,
    variable_list,
    LLM_model = "gpt-4",
    update_key = FALSE,
    n_perm = NULL,
    seed = 123,
    main_prompt = NULL,
    display_progress = TRUE,
    logprobs = TRUE
) {
  # ---------- helpers ----------
  `%||%` <- function(x, y) if (is.null(x)) y else x

  model_supports_logprobs <- function(m) {
    grepl("^gpt-4o$|^gpt-4-turbo$|^gpt-4$|^gpt-3\\.5-turbo$", m)
  }

  extract_decision_char <- function(txt) {
    ch <- substr(trimws(txt %||% ""), 1, 1)
    if (!nzchar(ch)) return("?")
    ch <- tolower(ch)
    if (ch %in% c("i", "e")) ch else "?"
  }

  # ---------- validation ----------
  stopifnot(is.character(context) | is.null(context))
  stopifnot(is.vector(variable_list) && length(variable_list) >= 3)
  stopifnot(all(sapply(variable_list, is.character)))

  # ---------- all unordered pairs ----------
  pairs_df <- data.frame(var1 = character(), var2 = character())
  for (i in 1:(length(variable_list) - 1)) {
    for (j in (i + 1):length(variable_list)) {
      pairs_df <- rbind(pairs_df, data.frame(var1 = variable_list[[i]], var2 = variable_list[[j]]))
    }
  }
  n_pairs <- nrow(pairs_df)

  # ---------- permutations count ----------
  if (missing(n_perm)) {
    n_perm <- 2
    message("The n_perm argument was not specified. The function will proceed using two permutations of the remaining variables.")
  }
  if (!missing(n_perm) && n_perm == 0) stop("n_perm cannot be zero.")
  if (n_perm > 50) stop("Requested `n_perm` (", n_perm, ") exceeds maximum possible permutations which is set to 50.")

  set.seed(seed)

  # ---------- prompts ----------
  system_prompt <- if (!is.null(main_prompt)) {
    paste(main_prompt, collapse = " ")
  } else {
    "You are an expert in using graphical models to study psychological constructs. \
You will classify whether there is a conditional relationship between pairs of variables in a Markov random field graphical model applied to psychological research. \
If there is a conditional association (edge present), output 'I'. If there is no conditional association (edge absent), output 'E'. \
Only output a single character: 'I' or 'E'. Consider the remaining variables."
  }

  # ---------- config ----------
  use_logprobs <- isTRUE(logprobs) && model_supports_logprobs(LLM_model)

  # ---------- storage ----------
  raw_LLM        <- vector("list", n_pairs)
  logprobs_LLM   <- vector("list", n_pairs)
  mode_used_mat  <- matrix("fallback", nrow = n_pairs, ncol = n_perm)  # "logprobs" or "fallback"

  # ---------- main loop over pairs ----------
  for (i in 1:n_pairs) {
    var1 <- pairs_df[i, 1]
    var2 <- pairs_df[i, 2]
    remaining_vars <- setdiff(variable_list, c(var1, var2))

    # Build list of permutations for remaining variables
    if (length(remaining_vars) >= 2) {
      perms <- matrix(nrow = 0, ncol = length(remaining_vars))
      while (nrow(perms) < n_perm) {
        new_perm <- sample(remaining_vars, length(remaining_vars), replace = FALSE)
        if (nrow(perms) == 0 || !any(apply(perms, 1, function(x) all(x == new_perm)))) {
          perms <- rbind(perms, new_perm)
        }
      }
      remaining_vars_list <- lapply(1:n_perm, function(p) perms[p, ])
    } else if (length(remaining_vars) == 1 && n_perm == 1) {
      remaining_vars_list <- list(remaining_vars)
    } else {
      remaining_vars_list <- replicate(n_perm, remaining_vars, simplify = FALSE)
    }

    raw_LLM_pair      <- vector("list", n_perm)  # pre-allocate
    logprobs_LLM_pair <- vector("list", n_perm)  # pre-allocate

    for (perm in 1:n_perm) {
      remaining_vars_str <- paste(remaining_vars_list[[perm]], collapse = ", ")
      prompt <- paste0(
        if (!is.null(context)) paste0("Context: ", context, "\n") else "",
        "Current pair: '", var1, "' & '", var2, "'\n",
        "Remaining variables: ", remaining_vars_str, "\n",
        "Respond ONLY with 'I' (included) or 'E' (excluded). Do not write anything else."
      )

      if (isTRUE(display_progress)) {
        message(paste0("Processing pair ", i, "/", n_pairs, ", permutation ", perm, ": ", var1, " - ", var2))
      }

      # ---- LLM call ----
      LLM_output <- callLLM(
        prompt        = prompt,
        LLM_model     = LLM_model,
        max_tokens    = 1,                # callLLM handles GPT-5 specifics internally
        temperature   = 0,
        logprobs      = use_logprobs,
        raw_output    = TRUE,
        system_prompt = system_prompt,
        update_key    = update_key
      )
      update_key <- FALSE

      content_txt <- LLM_output$output %||% LLM_output$raw_content$content %||% ""

      raw_LLM_pair[[perm]] <- c(
        pair_index    = i,
        permutation   = perm,
        prompt        = prompt,
        system_prompt = system_prompt,
        LLM_model     = LLM_output$raw_content$LLM_model %||% LLM_model,
        content       = content_txt,
        finish_reason = LLM_output$raw_content$finish_reason %||% NA_character_,
        prompt_tokens = LLM_output$raw_content$prompt_tokens %||% NA_integer_,
        answer_tokens = LLM_output$raw_content$answer_tokens %||% NA_integer_,
        total_tokens  = LLM_output$raw_content$total_tokens  %||% NA_integer_,
        error         = LLM_output$raw_content$error %||% NA
      )

      if (use_logprobs && !is.null(LLM_output$top5_tokens)) {
        logprobs_LLM_pair[[perm]] <- LLM_output$top5_tokens
        mode_used_mat[i, perm] <- "logprobs"
      } else {
        logprobs_LLM_pair[[perm]] <- NULL
        mode_used_mat[i, perm] <- "fallback"
      }
    }

    raw_LLM[[i]]      <- raw_LLM_pair
    logprobs_LLM[[i]] <- logprobs_LLM_pair
  }

  # ---------- probability aggregation (guarded) ----------
  prob_matrix  <- matrix(NA_real_, nrow = n_pairs, ncol = n_perm)
  n_default_05 <- 0

  for (i in 1:n_pairs) {
    for (perm in 1:n_perm) {

      # Safely get first-token logprobs df if present
      first_token_df <- NULL
      if (!is.null(logprobs_LLM[[i]]) &&
          length(logprobs_LLM[[i]]) >= perm &&
          !is.null(logprobs_LLM[[i]][[perm]]) &&
          length(logprobs_LLM[[i]][[perm]]) > 0 &&
          !is.null(logprobs_LLM[[i]][[perm]][[1]])) {
        first_token_df <- logprobs_LLM[[i]][[perm]][[1]]
      }

      if (!is.null(first_token_df)) {
        prob_i <- 0; prob_e <- 0
        for (m in 1:nrow(first_token_df)) {
          token <- trimws(tolower(first_token_df$top5_tokens[m]))
          if (token == "i") prob_i <- prob_i + as.numeric(first_token_df$probability[m])
          if (token == "e") prob_e <- prob_e + as.numeric(first_token_df$probability[m])
        }
        if (prob_i + prob_e > 0) {
          prob_matrix[i, perm] <- prob_i / (prob_i + prob_e)
        } else {
          prob_matrix[i, perm] <- 0.5; n_default_05 <- n_default_05 + 1
        }
      } else {
        temp_text <- raw_LLM[[i]][[perm]][["content"]]
        temp_text <- if (is.null(temp_text)) "" else temp_text
        dchr <- tolower(substr(trimws(temp_text), 1, 1))
        if (dchr == "i") {
          prob_matrix[i, perm] <- 1
        } else if (dchr == "e") {
          prob_matrix[i, perm] <- 0
        } else {
          prob_matrix[i, perm] <- 0.5; n_default_05 <- n_default_05 + 1
        }
      }
    }
  }

  message(paste0("Number of edges defaulted to 0.5 (no usable signal): ",
                 n_default_05, " out of ", n_perm * n_pairs, " total."))

  avg_probs <- rowMeans(prob_matrix, na.rm = TRUE)
  prob_relation_df <- data.frame(
    var1 = pairs_df[, 1],
    var2 = pairs_df[, 2],
    prob = avg_probs,
    row.names = NULL
  )

  # ---------- flatten raw_LLM ----------
  output <- list()
  tryCatch({
    flattened_df_raw_LLM <- data.frame(
      pair_index = integer(),
      permutation = integer(),
      var1 = character(),
      var2 = character(),
      LLM_model = character(),
      prompt = character(),
      system_prompt = character(),
      content = character(),
      mode_used = character(),
      finish_reason = character(),
      prompt_tokens = numeric(),
      answer_tokens = numeric(),
      total_tokens = numeric(),
      error = character(),
      stringsAsFactors = FALSE
    )

    for (i in seq_along(raw_LLM)) {
      for (perm in seq_along(raw_LLM[[i]])) {
        temp <- raw_LLM[[i]][[perm]]
        flattened_df_raw_LLM <- rbind(
          flattened_df_raw_LLM,
          data.frame(
            pair_index    = as.integer(temp[["pair_index"]] %||% i),
            permutation   = as.integer(temp[["permutation"]] %||% perm),
            var1          = pairs_df[i, 1],
            var2          = pairs_df[i, 2],
            LLM_model     = temp[["LLM_model"]] %||% LLM_model,
            prompt        = temp[["prompt"]] %||% "",
            system_prompt = temp[["system_prompt"]] %||% "",
            content       = temp[["content"]] %||% "",
            mode_used     = mode_used_mat[i, perm],
            finish_reason = temp[["finish_reason"]] %||% NA_character_,
            prompt_tokens = as.numeric(temp[["prompt_tokens"]] %||% NA),
            answer_tokens = as.numeric(temp[["answer_tokens"]] %||% NA),
            total_tokens  = as.numeric(temp[["total_tokens"]] %||% NA),
            error         = ifelse(is.null(temp[["error"]]), NA, temp[["error"]]),
            stringsAsFactors = FALSE
          )
        )
      }
    }
    output$raw_LLM <- flattened_df_raw_LLM
  }, error = function(e) {
    cat(paste0("Warning: Unable to return raw LLM output -> ", e$message, "."),
        "Only part of the output is returned.", sep = "\n")
  })

  # ---------- diagnostics ----------
  diag_tbl <- table(mode_used_mat)
  output$diagnostics <- list(
    mode_counts = as.list(diag_tbl),
    defaults_0p5 = n_default_05
  )

  # ---------- assemble ----------
  output$relation_df <- prob_relation_df
  message(paste0("Total of LLM prompts: ", n_pairs * n_perm))
  output$arguments <- list(
    context = context,
    variable_list = variable_list,
    LLM_model = LLM_model,
    update_key = update_key,
    n_perm = n_perm,
    seed = seed,
    n_default_05 = n_default_05,
    logprobs_requested = isTRUE(logprobs),
    logprobs_used = isTRUE(use_logprobs)
  )
  class(output) <- "elicitEdgeProbLite"
  return(output)
}
