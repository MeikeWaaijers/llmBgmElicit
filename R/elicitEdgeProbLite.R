#' Elicit Edge Inclusion Priors for Variable Pairs Using a LLM
#' that takes into account the remaining variables
#'
#'
#' This function uses a large language model (LLM) to evaluate whether conditional
#' associations (edges) exist between variable pairs in a Markov random Field graphical model. It optionally
#' includes study context and considers different orderings of remaining variables in the network
#' to estimate the prior probability of edge inclusion. Unlike the `elicitEdgeProb` function,
#' it does not include the decisions from the previous iterations for the remaining edges
#' in the prompt and is therefore cheaper to user. However it is also less accurate.
#'
#' @param context Optional character string. Describes the research context to inform the LLM's evaluation.
#' @param variable_list A character vector with at least three variable names.
#' @param LLM_model A character string specifying the model to use. One of: `"gpt-4o"`, `"gpt-4-turbo"`,
#'   `"gpt-3.5-turbo"`, `"mixtral"`, or `"llama-3"`.
#' @param update_key Logical. If `TRUE`, updates the API key used for the LLM call. Default is `FALSE`.
#' @param n_perm Optional integer. Number of sampled permutations of remaining variables. Ignored for small sets.
#' If `NULL`, two permutations are generated by default. The maximum is 50.
#' @param seed Integer. Random seed for reproducibility of the permutations. Default is `123`.
#' @param main_prompt Optional vector of character strings to be used as a system prompt for the LLM.
#' If `NULL`, a default system prompt is used. The prompt should be a single string.
#' @param display_progress Logical. If `TRUE`, displays progress messages during processing. Default is `TRUE`.
#'
#' @return A list of class `"elicitEdgeProbLite"` containing:
#' \describe{
#'   \item{`relation_df`}{A data frame with columns `var1`, `var2`, and `prob`, containing the estimated
#'     prior probabilities of conditional associations between variable pairs.}
#'   \item{`raw_LLM`}{A data frame of all raw LLM prompt-response data and token probabilities.}
#'   \item{`arguments`}{A list of the function input arguments for reproducibility.}
#' }
#'
#' @details
#' The function examines all unique variable pairs and, for each, prompts the LLM
#' to assess the presence of a conditional association, factoring in remaining variables in different
#' permutations. If context is provided, it is included in the prompt to inform decisions.
#'
#' The returned edge inclusion probability reflects the average over all considered permutations
#' for each variable pair. The function handles both default and user-specified prompting structures.
#'
#' @examples
#' \dontrun{
#' result <- elicitEdgeProbLite(
#'   context = "This study examines the relationship between screen time,
#'   physical activity, and cardiovascular health.",
#'   variable_list = c("Screen Time", "Physical Activity", "Cardiovascular Health"),
#'   LLM_model = "gpt-4o"
#' )
#' print(result$relation_df)
#' }
#'
#' @export


elicitEdgeProbLite <- function(
    context,
    variable_list,
    LLM_model = "gpt-4",
    update_key = FALSE,
    n_perm = NULL,
    seed = 123,
    main_prompt = NULL,
    display_progress = TRUE
) {
  stopifnot(is.character(context) | is.null(context))
  stopifnot(is.vector(variable_list) && length(variable_list) >= 3)
  stopifnot(all(sapply(variable_list, is.character)))

  # Generate all pairs (same as in elicitEdgeProb)
  pairs_df <- data.frame(var1 = character(), var2 = character())
  for (i in 1:(length(variable_list) - 1)) {
    for (j in (i + 1):length(variable_list)) {
      pairs_df <- rbind(pairs_df, data.frame(var1 = variable_list[[i]], var2 = variable_list[[j]]))
    }
  }
  n_pairs <- nrow(pairs_df)
  if (missing(n_perm)) {
    n_perm <- 2
    message("The n_perm argument was not specified. The function will proceed using two permutations of the remaining variables.")
  }
  if (!missing(n_perm) && n_perm == 0) stop("n_perm cannot be zero.")
  if (n_perm > 50) stop("Requested `n_perm` (", n_perm, ") exceeds maximum possible permutations which is set to 50.")

  set.seed(seed)

  # specify the system prompt
  if(!is.null(main_prompt)) {
    system_prompt <- main_prompt
  } else{
  system_prompt <- "You are an expert in using graphical models to study psychological constructs. You will be asked to classify whether there is a conditional relationship between pairs of variables in a Markov random field graphical model, applied to psychological research. If a conditional association exists, it means that the variables remain related even after accounting for the relationships between the other variables in the network. However, if the other variables explain away the relation between the two variables, then an edge should be absent. Consider the example that the relation between ice cream sales and number of shark attacks disappears when we take into account the outside temperature. You must use your vast prior knowledge of the relationships between the variables to make informed decisions. When presented with two variable names, you should evaluate whether or not there is an edge between those two variables in the graphical model (which reflects a conditional association) between them, considering the remaining variables. If there is a conditional association between two variables, then the edge should be categorized as included (by outputting 'I'). If there is no conditional association, then the edge should be categorized as excluded (by outputting 'E'). Therefore, output should be either 'I' or 'E'! Do not include any additional explanation or other text. Since you must make decisions about conditional associations, be sure to consider the remaining variables when making your decision."
  }
  # Prepare storage
  raw_LLM <- list()
  logprobs_LLM <- list()

  for (i in 1:n_pairs) {
    var1 <- pairs_df[i, 1]
    var2 <- pairs_df[i, 2]
    remaining_vars <- setdiff(variable_list, c(var1, var2))

    # Generate n_perm permutations of remaining vars
    if (length(remaining_vars) >= 2) {
      perms <- matrix(nrow = 0, ncol = length(remaining_vars))
      while (nrow(perms) < n_perm) {
        new_perm <- sample(remaining_vars, length(remaining_vars), replace = FALSE)
        if (!any(apply(perms, 1, function(x) all(x == new_perm)))) {
          perms <- rbind(perms, new_perm)
        }
      }
      remaining_vars_list <- lapply(1:n_perm, function(p) perms[p, ])
    } else if (length(remaining_vars) == 1 && n_perm == 1) {
      remaining_vars_list <- list(remaining_vars)
    } else {
      remaining_vars_list <- replicate(n_perm, remaining_vars, simplify = FALSE)
    }

    raw_LLM_pair <- list()
    logprobs_LLM_pair <- list()
    for (perm in 1:n_perm) {
      remaining_vars_str <- paste(remaining_vars_list[[perm]], collapse = ", ")

      # PROMPT: built identically to the full function, always minimal
      prompt <- paste0(
        if (!is.null(context)) paste0("Context: ", context, "\n") else "",
        "Current pair: '", var1, "' & '", var2, "'\n",
        "Remaining variables: ", remaining_vars_str, "\n",
        "Respond ONLY with 'I' (included) or 'E' (excluded). Do not write anything else."
      )

      if(display_progress) {
        print(paste0("Processing pair ", i, "/", n_pairs, ", permutation ", perm, ": ", var1, " - ", var2))
      }

      # Call LLM
      LLM_output <- callLLM(
        prompt = prompt,
        LLM_model = LLM_model,
        max_tokens = 1,
        temperature = 0,
        logprobs = TRUE,
        raw_output = TRUE,
        system_prompt = system_prompt,
        update_key = update_key
      )
      update_key <- FALSE

      raw_LLM_pair[[perm]] <- c(prompt = prompt, system_prompt = system_prompt, LLM_output$raw_content)
      logprobs_LLM_pair[[perm]] <- LLM_output$top5_tokens
    }
    raw_LLM[[i]] <- raw_LLM_pair
    logprobs_LLM[[i]] <- logprobs_LLM_pair
  }

  # Probability processing: always use FIRST token logprobs for all models --
  prob_matrix <- matrix(NA, nrow = n_pairs, ncol = n_perm)
  valid_tokens <- c("i", "e")
  n_default_05 <- 0
  for (i in 1:n_pairs) {
    for (perm in 1:n_perm) {
      first_token_data <- logprobs_LLM[[i]][[perm]][[1]]
      prob_i <- 0
      prob_e <- 0
      for (m in 1:nrow(first_token_data)) {
        token <- trimws(tolower(first_token_data$top5_tokens[m]))
        if (token == "i") prob_i <- prob_i + as.numeric(first_token_data$probability[m])
        if (token == "e") prob_e <- prob_e + as.numeric(first_token_data$probability[m])
      }
      if (prob_i + prob_e > 0) {
        prob_matrix[i, perm] <- prob_i / (prob_i + prob_e)
      } else {
        prob_matrix[i, perm] <- 0.5
        n_default_05 <- n_default_05 + 1
      }
    }
  }

  print(paste0("Number of edges defaulted to 0.5 (neither 'I' nor 'E' found in first token's top5): ", n_default_05, " out of ", n_perm * n_pairs, " total."))


  avg_probs <- rowMeans(prob_matrix, na.rm = TRUE)
  prob_relation_df <- data.frame(
    var1 = pairs_df[, 1],
    var2 = pairs_df[, 2],
    prob = avg_probs,
    row.names = NULL
  )

  #  Output flattening (same style as full function) --
  output <- list()
  tryCatch({
    flattened_df_raw_LLM <- data.frame(
      pair_index = integer(),
      permutation = integer(),
      var1 = character(),
      var2 = character(),
      LLM_model = character(),
      prompt = character(),
      system_prompt = character(),
      content = character(),
      finish_reason = character(),
      prompt_tokens = numeric(),
      answer_tokens = numeric(),
      total_tokens = numeric(),
      error = character(),
      first_token_prob = numeric(),
      stringsAsFactors = FALSE
    )
    for (i in seq_along(raw_LLM)) {
      for (perm in seq_along(raw_LLM[[i]])) {
        temp <- raw_LLM[[i]][[perm]]
        first_token_info <- if (!is.null(logprobs_LLM[[i]][[perm]]) && length(logprobs_LLM[[i]][[perm]]) > 0) {
          first_token_data <- logprobs_LLM[[i]][[perm]][[1]]
          data.frame(
            first_token_prob = as.numeric(first_token_data$probability[1]),
            stringsAsFactors = FALSE
          )
        } else {
          data.frame(first_token_prob = NA_real_, stringsAsFactors = FALSE)
        }
        flattened_df_raw_LLM <- rbind(
          flattened_df_raw_LLM,
          data.frame(
            pair_index = i,
            permutation = perm,
            var1 = pairs_df[i, 1],
            var2 = pairs_df[i, 2],
            LLM_model = temp$LLM_model,
            prompt = temp$prompt,
            system_prompt = temp$system_prompt,
            content = temp$content,
            finish_reason = temp$finish_reason,
            prompt_tokens = temp$prompt_tokens,
            answer_tokens = temp$answer_tokens,
            total_tokens = temp$total_tokens,
            error = ifelse(is.null(temp$error), NA, temp$error),
            first_token_prob = first_token_info$first_token_prob,
            stringsAsFactors = FALSE
          )
        )
      }
    }
    output$raw_LLM <- flattened_df_raw_LLM
  }, error = function(e) {
    cat(paste0("Warning: Unable to return raw LLM output -> ", e$message, "."),
        "Only part of the output is returned.", sep = "\n")
  })

  output$relation_df <- prob_relation_df
  print(paste0("Total of LLM prompts: ", n_pairs * n_perm))
  output$arguments <- list(
    context = context,
    variable_list = variable_list,
    LLM_model = LLM_model,
    update_key = update_key,
    n_perm = n_perm,
    seed = seed,
    n_default_05 = n_default_05
  )
  class(output) <- "elicitEdgeProbLite"
  return(output)
}
